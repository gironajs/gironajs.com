---
title: Web Scraping d'ofertes de feina a LinkedIn utilitzant Puppeteer i RxJS
description: Tutorial sobre com extreure ofertes de feina de LinkedIn utilitzant Puppeteer i RxJS
image: /assets/blog/web-scraping-de-feines-a-linkedin-utilitzant-puppeteer-i-rxjs/cover-image.png
published: true
publishedDate: 2023-10-03
authors: llorenspujol
seo:
  metatitle: >
    Web Scraping d'ofertes de feina a LinkedIn utilitzant Puppeteer i RxJS
  metadescription: >
    Tutorial sobre com extreure ofertes de feina de LinkedIn utilitzant Puppeteer i RxJS
  image: >
    /assets/blog/web-scraping-de-feines-a-linkedin-utilitzant-puppeteer-i-rxjs/cover-image.png
---

El web scraping pot semblar una tasca senzilla, per√≤ hi ha molts reptes a superar. En aquest blog, aprendrem en com fer "scraping" a LinkedIn per extreure ofertes de feina. Per fer aix√≤, utilitzarem [Puppeteer](https://pptr.dev/) i [RxJS](https://rxjs.dev/). L'objectiu √©s assolir web scraping d'una manera declarativa, modular i escalable.

## Qu√® √©s el web scraping?

El web scraping √©s una t√®cnica d'extracci√≥ de dades utilitzada per recopilar informaci√≥ de llocs web. Consisteix en un proc√©s automatitzat d'obtenci√≥ de dades de p√†gines web, com ara text, imatges, enlla√ßos i m√©s, per a despr√©s emmagatzemar o processar aquestes dades per a diversos prop√≤sits.

### Puppeteer

[Puppeteer](https://pptr.dev/) √©s una biblioteca JavaScript que permet controlar navegadors web com Chrome per a web scraping. Puppeteer ens permet programar i monitorar tasques com ara navegar a una p√†gina web espec√≠fica i extreure les dades que necessitem. √âs l'eina ideal per a web scraping perqu√®, sent un navegador web, pot superar qualsevol obstacle potencial, com ara en el cas de llocs web que requereixen l'execuci√≥ de JavaScript per funcionar o mostrar dades.

### RxJS

[RxJS](https://rxjs.dev/) √©s una biblioteca per a la programaci√≥ reactiva en JavaScript. Proporciona un conjunt d'eines i abstraccions per treballar amb fluxos de dades as√≠ncrons. Utilitzarem RxJS en aquest exemple perqu√® ofereix els avantatges seg√ºents:
- Codi as√≠ncron declaratiu
- Millora de la gesti√≥ d'errors
- L√≤gica de reintent millorada
- Adaptaci√≥ del codi simplificada
- Una √†mplia gamma d'operadors per ajudar-nos al llarg del proc√©s


## 1. Inicialitzaci√≥ de Puppeteer

El fragment de codi a continuaci√≥ inicialitza una inst√†ncia de navegador Puppeteer en un mode no "headless" (es a dir, amb interf√≠cie gr√†fica) i posteriorment crea una nova p√†gina web. Aix√≤ representa el proc√©s d'inicialitzaci√≥ m√©s fonamental i directe per a Puppeteer:

```ts:src/index.ts
(async () => {
  console.log('Launching Chrome...');
  const browser = await puppeteer.launch({
    headless: false,
    // devtools: true,
    // slowMo: 250, // slow down puppeteer script so that it's easier to follow visually
    args: [
      '--disable-gpu',
      '--disable-dev-shm-usage',
      '--disable-setuid-sandbox',
      '--no-first-run',
      '--no-sandbox',
      '--no-zygote',
      '--single-process',
    ],
  });

  const page = await browser.newPage()

    /** 
     * 1. Go lo linkedin jobs url
     * 2. Get the jobs
     * 3. Repeat step 1 with other search parameters
     */
    
})();
```

> Alguns dels fragments en aquest blog poden ometre parts per claredat. Podeu trobar el codi complet en aquest [repositori](https://github.com/llorenspujol/linkedin-jobs-scraper).


## 2. Anar a la llista de llocs de treball de LinkedIn i extreure les dades

Aquesta √©s la part central d'aquest blog, on ens submergim en el proc√©s d'acc√©s a les ofertes de feina de LinkedIn, analitzant el contingut HTML i recuperant les dades d'ofertes de feina en format JSON.

### 2.1. Construeix l'URL per navegar fins a la p√†gina d'ofertes de feina de LinkedIn

Per accedir a les ofertes de feina de LinkedIn, necessitem construir una URL utilitzant la funci√≥ `urlQueryPage`:

```ts:src/linkedin.ts
export const urlQueryPage = (search: ScraperSearchParams) =>
    `https://linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=${search.searchText}
    &start=${search.nPage * 25}${search.locationText ? '&location=' + search.locationText : ''}`
```

En aquest cas, ja he realitzat la investigaci√≥ pr√®via per trobar aquesta URL. El nostre objectiu √©s trobar una URL que puguem parametritzar amb els nostres par√†metres de cerca desitjats.
Per a aquest exemple, els nostres par√†metres de cerca seran `searchText`, `pageNumber` i opcionalment `locationText`.

Exemples de url poden ser:
1. <a href="https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Angular&start=0" target="_blank">https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Angular&start=0</a>
2. <a href="https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=React&location=Barcelona&start=0" target="_blank">https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=React&location=Barcelona&start=0</a>
3. <a href="https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=python&start=0" target="_blank">https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=python&start=0</a>


### 2.2. Navega a l'URL i extreu les ofertes de feina

Amb la nostra URL identificada, podem procedir amb les dues accions principals requerides:

1. Navegar a la URL on hi ha les ofertes de feina: Aquest pas implica dirigir la nostra eina de "scraping" web a la URL on estan les ofertes de feina.

2. Extreure dades de les ofertes de feina i convertint-les a JSON: Un cop estem a la p√†gina d'ofertes de feina, utilitzarem t√®cniques de "scraping" web per extreure les dades de les ofertes i retornar-les en format JSON.

```ts:src/linkedin.ts

/** main function */
export function getJobsFromLinkedinPage(page: Page, searchParams): Observable<JobInterface[]> {
    return defer(() => navigateToJobsPage(page, searchParams))
        .pipe(switchMap(() => getJobsFromLinkedinPage(page)));
}

/* Utility functions  */

export const urlQueryPage = (search: ScraperSearchParams) =>
    `https://linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=${search.searchText}
    &start=${search.nPage * 25}${search.locationText ? '&location=' + search.locationText : ''}`

function navigateToJobsPage(page: Page, searchParams): Promise<Response | null> {
    return page.goto(urlQueryPage(searchParams), { waitUntil: 'networkidle0' });
}

export const stacks = ['angularjs', 'kubernetes', 'javascript', 'jenkins', 'html', /* ... */];

export function getJobsFromLinkedinPage(page: Page): Observable<JobInterface[]> {
    return defer(() => fromPromise(page.evaluate((pageEvalData) => {
        const collection: HTMLCollection = document.body.children;
        const results: JobInterface[] = [];
        for (let i = 0; i < collection.length; i++) {
            try {
                const item = collection.item(i)!;
                const title = item.getElementsByClassName('base-search-card__title')[0].textContent!.trim();
                const imgSrc = item.getElementsByTagName('img')[0].getAttribute('data-delayed-url') || '';
                const remoteOk: boolean = !!title.match(/remote|No office location/gi);

                const url = (
                    (item.getElementsByClassName('base-card__full-link')[0] as HTMLLinkElement)
                    || (item.getElementsByClassName('base-search-card--link')[0] as HTMLLinkElement)
                ).href;

                const companyNameAndLinkContainer = item.getElementsByClassName('base-search-card__subtitle')[0];
                const companyUrl: string | undefined = companyNameAndLinkContainer?.getElementsByTagName('a')[0]?.href;
                const companyName = companyNameAndLinkContainer.textContent!.trim();
                const companyLocation = item.getElementsByClassName('job-search-card__location')[0].textContent!.trim();

                const toDate = (dateString: string) => {
                    const [year, month, day] = dateString.split('-')
                    return new Date(parseFloat(year), parseFloat(month) - 1, parseFloat(day)    )
                }

                const dateTime = (
                    item.getElementsByClassName('job-search-card__listdate')[0]
                    || item.getElementsByClassName('job-search-card__listdate--new')[0] // less than a day. TODO: Improve precision on this case.
                ).getAttribute('datetime');
                const postedDate = toDate(dateTime as string).toISOString();


                /**
                 * Calculate minimum and maximum salary
                 *
                 * Salary HTML example to parse:
                 * <span class="job-result-card__salary-info">$65,000.00 - $90,000.00</span>
                 */
                let currency: SalaryCurrency = ''
                let salaryMin = -1;
                let salaryMax = -1;

                const salaryCurrencyMap: any = {
                    ['‚Ç¨']: 'EUR',
                    ['$']: 'USD',
                    ['¬£']: 'GBP',
                }

                const salaryInfoElem = item.getElementsByClassName('job-search-card__salary-info')[0]
                if (salaryInfoElem) {
                    const salaryInfo: string = salaryInfoElem.textContent!.trim();
                    if (salaryInfo.startsWith('‚Ç¨') || salaryInfo.startsWith('$') || salaryInfo.startsWith('¬£')) {
                        const coinSymbol = salaryInfo.charAt(0);
                        currency = salaryCurrencyMap[coinSymbol] || coinSymbol;
                    }

                    const matches = salaryInfo.match(/([0-9]|,|\.)+/g)
                    if (matches && matches[0]) {
                        // values are in USA format, so we need to remove ALL the comas
                        salaryMin = parseFloat(matches[0].replace(/,/g, ''));
                    }
                    if (matches && matches[1]) {
                        // values are in USA format, so we need to remove ALL the comas
                        salaryMax = parseFloat(matches[1].replace(/,/g, ''));
                    }
                }

                // Calculate tags
                let stackRequired: string[] = [];
                title.split(' ').concat(url.split('-')).forEach(word => {
                    if (!!word) {
                        const wordLowerCase = word.toLowerCase();
                        if (pageEvalData.stacks.includes(wordLowerCase)) {
                            stackRequired.push(wordLowerCase)
                        }
                    }
                })
                // Define uniq function here. remember that page.evaluate executes inside the browser, so we cannot easily import outside functions form other contexts
                const uniq = (_array) => _array.filter((item, pos) => _array.indexOf(item) == pos);
                stackRequired = uniq(stackRequired)

                const result: JobInterface = {
                    id: item!.children[0].getAttribute('data-entity-urn') as string,
                    city: companyLocation,
                    url: url,
                    companyUrl: companyUrl || '',
                    img: imgSrc,
                    date: new Date().toISOString(),
                    postedDate: postedDate,
                    title: title,
                    company: companyName,
                    location: companyLocation,
                    salaryCurrency: currency,
                    salaryMax: salaryMax,
                    salaryMin: salaryMin,
                    countryCode: '',
                    countryText: '',
                    descriptionHtml: '',
                    remoteOk: remoteOk,
                    stackRequired: stackRequired
                };
                console.log('result', result);

                results.push(result);
            } catch (e) {
                console.error(`Something when wrong retrieving linkedin page item: ${i} on url: ${window.location}`, e.stack);
            }
        }
        return results;
    }, {stacks})) as Observable<JobInterface[]>)
}

```

El codi proporcionat extreu tota la informaci√≥ de les ofertes de feina de la p√†gina. Encara que el codi no √©s molt est√®tic, aconsegueix la feina, que √©s t√≠pic per a codi de "scraping" web.

> En un context de programaci√≥ normal, generalment √©s aconsellable descompondre el codi en funcions m√©s petites i a√Øllades per millorar-ne la llegibilitat i la mantenibilitat. No obstant aix√≤, quan es tracta de codi executat dins de `page.evaluate` en Puppeteer, estem una mica limitats perqu√® aquest codi s'executa en la inst√†ncia de Puppeteer (Chrome), no en el nostre entorn Node.js. Per tant, tot el codi ha de ser autocontingut dins de la crida de `page.evaluate`. L'√∫nica excepci√≥ aqu√≠ s√≥n les variables (com `stacks` en el nostre cas), que poden passar-se com a arguments a `page.evaluate`, sempre que no continguin funcions o objectes complexos que no es puguin serialitzar.

En aquest cas, l'√∫nic component desafiant per passar de HTML text a JSON √©s la informaci√≥ del salari, ja que implica convertir un format de text com '$65,000.00 - $90,000.00' en dos valors num√®rics de salari m√≠nim i m√†xim separats.
A m√©s, hem encapsulat tot el codi dins d'un bloc try/catch per gestionar els errors de manera elegant. Encara que actualment registrem els errors a la consola, √©s aconsellable considerar la implementaci√≥ d'un mecanisme per emmagatzemar aquests registres d'error en disc. Aquesta pr√†ctica es torna particularment important perqu√® les p√†gines web sovint experimenten canvis, cosa que necessita actualitzacions freq√ºents al codi de l'an√†lisi HTML.


Finalment, √©s important remarcar que sempre utilitzem els operadors `defer` i `fromPromise` per convertir Promeses en Observables:

```typescript
defer(() => fromPromise(myPromise()))
```

Aquesta estrat√®gia √©s la m√©s recomanada ja que funciona de manera fiable en tots els escenaris. Les Promeses s√≥n "eager", mentre que els Observables s√≥n "lazy" i nom√©s s'inicien quan alg√∫ s'hi subscriu. L'operador `defer` ens permet convertir a "lazy" una Promesa. Per m√©s informaci√≥ pots anar a aquest [enlla√ß](https://stackoverflow.com/questions/39319279/convert-promise-to-observable/69360357#69360357)


## 3. Afegeix un bucle as√≠ncron per iterar a trav√©s de totes les p√†gines

En el pas anterior, hem apr√®s com obtenir totes les dades de les ofertes de feina d'una p√†gina de LinkedIn. Ara, el que volem fer √©s utilitzar aquest codi tantes vegades com sigui possible per recopilar tantes dades com puguem. Per aconseguir-ho, primer necessitem iterar a trav√©s de totes les p√†gines disponibles:


```ts:src/linkedin.ts
function getJobsFromAllPages(page: Page, initSearchParams: ScraperSearchParams): Observable<ScraperResult> {
    const getJobs$ = (searchParams: ScraperSearchParams) => goToLinkedinJobsPageAndExtractJobs(page, searchParams).pipe(
        map((jobs): ScraperResult => ({jobs, searchParams} as ScraperResult)),
        catchError(error => {
            console.error(error);
            return of({jobs: [], searchParams: searchParams})
        })
    );

    return getJobs$(initSearchParams).pipe(
        expand(({jobs, searchParams}) => {
            console.log(`Linkedin - Query: ${searchParams.searchText}, Location: ${searchParams.locationText}, Page: ${searchParams.pageNumber}, nJobs: ${jobs.length}, url: ${urlQueryPage(searchParams)}`);
            if (jobs.length === 0) {
                return EMPTY;
            } else {
                return getJobs$({...searchParams, pageNumber: searchParams.pageNumber + 1});
            }
        })
    );
}
```
El codi anterior incrementa el n√∫mero de p√†gina fins que arribem a una p√†gina on no hi ha ofertes de feina (que seria l'√∫ltima p√†gina). Per realitzar aquest bucle en RxJS, utilitzem l'operador `expand`, que projecta recursivament cada valor de la font (source) a un Observable que es fusiona en l'Observable de sortida. La seva funcionalitat est√† ben explicada [aqu√≠](https://ncjamieson.com/understanding-expand/).

> En RxJS, no podem utilitzar un bucle amb la paraula clau `for` com ho fem amb await/async. Hem d'utilitzar un bucle recursiu en el seu lloc. Encara que inicialment pugui semblar una limitaci√≥, en un context as√≠ncron, aquest m√®tode resulta ser m√©s avantatj√≥s en nombroses situacions

Aix√≠ doncs, com seria el codi equivalent utilitzant Promeses? Aqu√≠ en tenim un exemple:

```typescript
export async function getJobsFromAllPages(page: Page, searchParams: ScraperSearchParams): Promise<ScraperResult> {
    const results: ScraperResult = { jobs: [], searchParams };

    try {
        while (true) {
            const jobs = await getJobsFromLinkedinPage(page, searchParams);
            console.log(`Linkedin - Query: ${searchParams.searchText}, Location: ${searchParams.locationText}, Page: ${searchParams.nPage}, nJobs: ${jobs.length}, url: ${urlQueryPage(searchParams)}`);

            results.jobs.push(...jobs);

            if (jobs.length === 0) {
                break;
            }

            searchParams.nPage++;
        }
    } catch (error) {
        console.error('Error:', error);
        results.jobs = []; // Clear the jobs in case of an error.
    }

    return results;
}

```
Aquest codi √©s gaireb√© equivalent al basat en Observables, amb una difer√®ncia cr√≠tica: nom√©s emet quan totes les p√†gines han acabat de processar. En canvi, la implementaci√≥ utilitzant Observables emet despr√©s de cada p√†gina. Crear un "stream" √©s crucial en aquest cas perqu√® volem gestionar les ofertes de feina tan aviat siguin resoltes.

Certament, podr√≠em introduir la nostra l√≤gica despr√©s de la l√≠nia:

```typescript
const jobs = await getJobsFromLinkedinPage(page, searchParams);

/* Handle the jobs here */
```
...per√≤ aix√≤ acoblaria innecess√†riament el nostre codi de "scraping" amb la part que gestiona les dades de les ofertes de feina. Gestionar les dades de les ofertes pot implicar algunes transformacions, crides a alguna API, i finalment, guardar les dades a una base de dades.

Aix√≠ doncs, en aquest exemple veiem clarament un dels molts avantatges que els Observables ofereixen respecte a les Promeses.


## 4. Afegim un altre bucle as√≠ncron per iterar a trav√©s de tots els par√†metres de cerca especificats
Ara que sabem com iterar a trav√©s de totes les p√†gines, podem passar a l'√∫ltim pas: crear un bucle per iterar a trav√©s de diferents par√†metres de cerca.

Per aconseguir-ho, primer definirem l'estructura de dades en la qual emmagatzemarem aquests par√†metres de cerca i la denominarem `searchParamsList`:

```ts:src/data.ts
const searchParamsList: { searchText: string; locationText: string }[] = [
  { searchText: 'Angular', locationText: 'Barcelona' },
  { searchText: 'Angular', locationText: 'Madrid' },
  // ...
  { searchText: 'React', locationText: 'Barcelona' },
  { searchText: 'React', locationText: 'Madrid' },
  // ...
];
```

Per iterar as√≠ncronament a trav√©s de l'array `searchParamsList`, b√†sicament necessitem convertir-lo d'un Array a un Observable utilitzant l'operador `fromArray`. Subseq√ºentment, utilitzarem l'operador `concatMap` per processar de manera seq√ºencial cada parell de searchText i locationText. La for√ßa de RxJS aqu√≠ √©s que, en el cas que de voler canviar de processament seq√ºencial a paral¬∑lel, simplement hem de canviar el `concatMap` per un `mergeMap`. En aquest cas no es recomana perqu√® superariem el l√≠mit de peticions (per temps) de LinkedIn, per√≤ √©s una cosa a tenir en compte en altres escenaris.

```ts:src/linkedin.ts
/**
 * Creates a new page and scrapes LinkedIn job data for each pair of searchText and locationText, recursively retrieving data until there are no more pages.
 * @param browser A Puppeteer instance
 * @returns An Observable that emits scraped job data as ScraperResult
 */
export function getJobsFromLinkedin(browser: Browser): Observable<ScraperResult> {
    // Create a new page
    const createPage = defer(() => fromPromise(browser.newPage()));

    // Iterate through search parameters and scrape jobs
    const scrapeJobs = (page: Page): Observable<ScraperResult> =>
        fromArray(searchParamsList).pipe(
            concatMap(({ searchText, locationText }) =>
                getJobsFromPageRecursive(page, { searchText, locationText, nPage: 0 })
            )
        )

    // Compose sequentially previous steps
    return createPage.pipe(switchMap(page => scrapeJobs(page)));
}
```


Aquest codi iterar√† a trav√©s de diversos par√†metres de cerca, un a la vegada, i recuperar√† ofertes de feina per a cada combinaci√≥ de `searchText` i `locationText`.

**üéâ Felicitats! Ara ja sou capa√ßos de fer "scraping" a LinkedIn i a qualsevol altre p√†gia web! üéâ**

Tot i aix√≤, LinkedIn, igual que moltes altres p√†gines web, t√© t√®cniques per prevenir el web scraping. Anem a veure com solventar-les üëá.


## Errors comuns al fer "scraping" a LinkedIn

Si executem el codi proporcionat, r√†pidament ens trobarem amb nombrosos errors de LinkedIn, fent dif√≠cil fer "scraping" amb √®xit d'una quantitat significativa d'informaci√≥. Hi ha dos errors comuns que necessitem abordar:

### 1. Resposta "status code" 429
Aquesta resposta pot passar durant el scraping, i significa que estem fent massa sol¬∑licituds en un petit per√≠ode de temps. Si ens trobem amb aquest error hem de reduir la velocitat en qu√® es duen a terme les peticions fins que desaparegui.

### 2. Authwall de LinkedIn
De tant en tant, LinkedIn ens pot redirigir a un authwall en lloc de la p√†gina desitjada. Quan apareix l'authwall, l'√∫nica opci√≥ √©s esperar una mica m√©s abans de fer la pr√≤xima sol¬∑licitud.

### Com superar aquests errors de manera efectiva

Aquests errors han de ser gestionats a la funci√≥ `getJobsFromLinkedinPage`, ampliem aquesta funci√≥ i separarem el codi de "scraping" html en una altra funci√≥ anomenada `getLinkedinJobsFromJobsPage`. El codi √©s aix√≠:

```ts:src/linkedin.ts
const AUTHWALL_PATH = 'linkedin.com/authwall';
const STATUS_TOO_MANY_REQUESTS = 429;
const JOB_SEARCH_SELECTOR = '.job-search-card';

function goToLinkedinJobsPageAndExtractJobs(page: Page, searchParams: ScraperSearchParams): Observable<JobInterface[]> {
    return defer(() => fromPromise(page.setExtraHTTPHeaders({'accept-language': 'en-US,en;q=0.9'})))
        .pipe(
            switchMap(() => navigateToLinkedinJobsPage(page, searchParams)),
            tap(response => checkResponseStatus(response)),
            switchMap(() => throwErrorIfAuthwall(page)),
            switchMap(() => waitForJobSearchCard(page)),
            switchMap(() => getJobsFromLinkedinPage(page)),
            retryWhen(retryStrategyByCondition({
                maxRetryAttempts: 4,
                retryConditionFn: error => error.retry === true
            })),
            map(jobs =>  Array.isArray(jobs) ? jobs : []),
            take(1)
        );
}

/**
 * Navigate to the LinkedIn search page, using the provided search parameters.
 */
function navigateToLinkedinJobsPage(page: Page, searchParams: ScraperSearchParams) {
    return defer(() => fromPromise(page.goto(urlQueryPage(searchParams), {waitUntil: 'networkidle0'})));
}

/**
 * Check the HTTP response status and throw an error if too many requests have been made.
 */
function checkResponseStatus(response: any) {
    const status = response?.status();
    if (status === STATUS_TOO_MANY_REQUESTS) {
        throw {message: 'Status 429 (Too many requests)', retry: true, status: STATUS_TOO_MANY_REQUESTS};
    }
}

/**
 * Check if the current page is an authwall and throw an error if it is.
 */
function throwErrorIfAuthwall(page: Page) {
    return getPageLocationOperator(page).pipe(tap(locationHref => {
        if (locationHref.includes(AUTHWALL_PATH)) {
            console.error('Authwall error');
            throw {message: `Linkedin authwall! locationHref: ${locationHref}`, retry: true};
        }
    }));
}

/**
 * Wait for the job search card to be visible on the page, and handle timeouts or authwalls.
 */
function waitForJobSearchCard(page: Page) {
    return defer(() => fromPromise(page.waitForSelector(JOB_SEARCH_SELECTOR, {visible: true, timeout: 5000}))).pipe(
        catchError(error => throwErrorIfAuthwall(page).pipe(tap(() => {throw error})))
    );
}
```

En aquest codi, abordem els errors esmentats anteriorment, √©s a dir, l'error de resposta 429 i el problema de l'authwall. Superar aquests errors √©s molt important per tenir √®xit durant l'scraping web a LinkedIn.

Per gestionar aquests errors, el codi utilitza una estrat√®gia de reintents personalitzada implementada per la funci√≥ [`retryStrategyByCondition`](https://github.com/llorenspujol/linkedin-jobs-scraper/blob/12b48449d773800bada82cbbfc09f76af5ac9289/src/scraper.utils.ts#L33):

```ts:src/scraper.utils.ts
export const retryStrategyByCondition = ({maxRetryAttempts = 3, scalingDuration = 1000, retryConditionFn = (error) => true}: {
    maxRetryAttempts?: number,
    scalingDuration?: number,
    retryConditionFn?: (error) => boolean
} = {}) => (attempts: Observable<any>) => {
    return attempts.pipe(
        mergeMap((error, i) => {
            const retryAttempt = i + 1;
            if (
                retryAttempt > maxRetryAttempts ||
                !retryConditionFn(error)
            ) {
                return throwError(error);
            }
            console.log(
                `Attempt ${retryAttempt}: retrying in ${retryAttempt *
                scalingDuration}ms`
            );
            // retry after 1s, 2s, etc...
            return timer(retryAttempt * scalingDuration);
        }),
        finalize(() => console.log('retryStrategyOnlySpecificErrors - finalized'))
    );
};
```

Aquesta estrat√®gia augmenta el temps entre cada intent despr√©s d'un error. D'aquesta manera, ens assegurem que esperarem prou temps perqu√® LinkedIn ens permeti fer sol¬∑licituds de nou

> Nota: √âs important ser conscient que LinkedIn pot posar a la llista negra la nostra adre√ßa IP, i simplement esperar m√©s temps pot no ser una soluci√≥ efectiva. Per mitigar aquest problema potencial i reduir els errors, una pr√†ctica recomanada √©s fer una rotaci√≥ d'IPs de tant en tant.


## Paraules finals

El web scraping pot violar freq√ºentment els termes de servei d'un lloc web. Sempre reviseu i respecteu l'arxiu robots.txt d'un lloc web i els seus Termes de Servei. En aquest cas, **aquest codi ha de ser utilitzat NOM√âS amb fins docents i de hobby**. LinkedIn prohibeix espec√≠ficament qualsevol extracci√≥ de dades del seu lloc web; podeu llegir m√©s [aqu√≠](https://www.linkedin.com/legal/crawling-terms).

Recomano utilitzar el web scraping per a l'aprenentatge, l'ensenyament i projectes de hobby. Sempre recordeu de respectar el lloc web, eviteu llan√ßar massa sol¬∑licituds i assegureu-vos que les dades s'utilitzen de manera respectuosa.

Tot el codi actualitzat es troba en aquest [repositori](https://github.com/llorenspujol/linkedin-jobs-scraper), no dubtis a donar-li una estrella si t'ha ajudat! üôè‚≠ê

Pots trobar-me a [Twitter](https://twitter.com/llorenspujol), [Linkedin](https://www.linkedin.com/in/lloren%C3%A7-pujol-ferriol-46575bb9/) o [Github](https://github.com/llorenspujol).

Tamb√©, gr√†cies a [aleix10kst](https://github.com/aleix10kst) per ajudar en el proc√©s de revisi√≥ del blog üôå.

Bon Scraping!üï∑

