---
title: Web Scraping d'ofertes de treball a LinkedIn utilitzant Puppeteer i RxJS
description: Tutorial sobre com extreure ofertes de treball de LinkedIn utilitzant Puppeteer i RxJS
image: /assets/blog/web-scraping-de-feines-a-linkedin-utilitzant-puppeteer-i-rxjs/cover-image.png
published: true
publishedDate: 2023-10-03
authors: llorenspujol
seo:
  metatitle: >
    Web Scraping d'ofertes de treball a LinkedIn utilitzant Puppeteer i RxJS
  metadescription: >
    Tutorial sobre com extreure ofertes de treball de LinkedIn utilitzant Puppeteer i RxJS
  image: >
    /assets/blog/web-scraping-de-feines-a-linkedin-utilitzant-puppeteer-i-rxjs/cover-image.png
---

El web scraping pot semblar una tasca senzilla, per√≤ hi ha molts reptes a superar. En aquest blog, ens endinsarem en com fer scraping a LinkedIn per extreure llistats de treball. Per fer aix√≤, utilitzarem [Puppeteer](https://pptr.dev/) i [RxJS](https://rxjs.dev/). L'objectiu √©s assolir web scraping d'una manera declarativa, modular i escalable.

## Entenent el Web scraping

El web scraping √©s una t√®cnica d'extracci√≥ de dades utilitzada per recopilar informaci√≥ de llocs web. Involucra el proc√©s automatitzat d'obtenci√≥ de dades espec√≠fiques de p√†gines web, com ara text, imatges, enlla√ßos i m√©s, i despr√©s emmagatzemar o processar aquestes dades per a diversos prop√≤sits.

### Puppeteer

[Puppeteer](https://pptr.dev/) √©s una biblioteca JavaScript que permet controlar navegadors web com Chrome per a web scraping. Puppeteer ens permet programar i monitorar tasques com ara navegar a una p√†gina web espec√≠fica i extreure les dades que necessitem. √âs l'eina ideal per a web scraping perqu√®, sent un navegador web, pot superar qualsevol obstacle potencial, com ara en el cas de llocs web que requereixen l'execuci√≥ de JavaScript per funcionar o mostrar dades.

### RxJS

[RxJS](https://rxjs.dev/) √©s una biblioteca per a la programaci√≥ reactiva en JavaScript. Proporciona un conjunt d'eines i abstraccions per treballar amb fluxos de dades asincr√≤nics. Utilitzarem RxJS en aquest exemple perqu√® ofereix els avantatges seg√ºents:
- Codi asincr√≤nic declaratiu
- Millora de la gesti√≥ d'errors
- L√≤gica de reintent millorada
- Adaptaci√≥ del codi simplificada
- Una √†mplia gamma d'operadors per ajudar-nos al llarg del proc√©s


## Inicialitzaci√≥ de Puppeteer

El fragment de codi a continuaci√≥ inicialitza una inst√†ncia de navegador Puppeteer en un mode no "headless" (es a dir, amb interf√≠cie gr√†fica) i posteriorment crea una nova p√†gina web. Aix√≤ representa el proc√©s d'inicialitzaci√≥ m√©s fonamental i directe per a Puppeteer:

```ts:src/index.ts
(async () => {
  console.log('Launching Chrome...');
  const browser = await puppeteer.launch({
    headless: false,
    // devtools: true,
    // slowMo: 250, // slow down puppeteer script so that it's easier to follow visually
    args: [
      '--disable-gpu',
      '--disable-dev-shm-usage',
      '--disable-setuid-sandbox',
      '--no-first-run',
      '--no-sandbox',
      '--no-zygote',
      '--single-process',
    ],
  });

  const page = await browser.newPage()

    /** 
     * 1. Go lo linkedin jobs url
     * 2. Get the jobs
     * 3. Repeat step 1 with other search parameters
     */
    
})();
```

> Alguns dels fragments en aquest blog poden ometre parts per claredat. Podeu trobar el codi complet en aquest [repositori](https://github.com/llorenspujol/linkedin-jobs-scraper).

This translation maintains the meaning of the original English text in Catalan.

## Anar a la llista de llocs de treball de LinkedIn i extreure les dades

Aquesta √©s la part central d'aquest bloc, on ens submergim en el proc√©s d'acc√©s a les ofertes de treball de LinkedIn, analitzant el contingut HTML i recuperant les dades del lloc de treball en format JSON.

### 1- Construint la URL per Navegar per les Ofertes de Treball de LinkedIn

Per accedir a les ofertes de treball de LinkedIn, necessitem construir una URL utilitzant la funci√≥ `urlQueryPage`:

```ts:src/linkedin.ts
export const urlQueryPage = (search: ScraperSearchParams) =>
    `https://linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=${search.searchText}
    &start=${search.nPage * 25}${search.locationText ? '&location=' + search.locationText : ''}`
```

Aquesta funci√≥ de generaci√≥ de URL √©s un pas crucial en el nostre proc√©s, ja que ens permet navegar per les ofertes de treball de LinkedIn amb els criteris de cerca espec√≠fics definits per `searchText`, `pageNumber`, i opcionalment `locationText`.

Exemples de url poden ser:
1. <a href="https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Angular&start=0" target="_blank">https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Angular&start=0</a>
2. <a href="https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=React&location=Barcelona&start=0" target="_blank">https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=React&location=Barcelona&start=0</a>
3. <a href="https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=python&start=0" target="_blank">https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=python&start=0</a>


### 2- Navegant a la URL i Extraient Dades de les Ofertes
Amb la nostra URL objectiu identificada, podem procedir amb les dues accions principals requerides:

1. Navegar a la URL de les Ofertes de Treball: Aquest pas implica dirigir la nostra eina de raspallat web a la URL on estan allotjades les ofertes de treball.

2. Extreure dades de les ofertes de feina i convertint-les a JSON: Un cop estem a la p√†gina d'ofertes de treball, utilitzarem t√®cniques de "scraping" web per extreure les dades de les ofertes i retornar-les en format JSON.

```ts:src/linkedin.ts

/** main function */
export function getJobsFromLinkedinPage(page: Page, searchParams): Observable<JobInterface[]> {
    return defer(() => navigateToJobsPage(page, searchParams))
        .pipe(switchMap(() => getJobsFromLinkedinPage(page)));
}

/* Utility functions  */

export const urlQueryPage = (search: ScraperSearchParams) =>
    `https://linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=${search.searchText}
    &start=${search.nPage * 25}${search.locationText ? '&location=' + search.locationText : ''}`

function navigateToJobsPage(page: Page, searchParams): Promise<Response | null> {
    return page.goto(urlQueryPage(searchParams), { waitUntil: 'networkidle0' });
}

export const stacks = ['angularjs', 'kubernetes', 'javascript', 'jenkins', 'html', /* ... */];

export function getJobsFromLinkedinPage(page: Page): Observable<JobInterface[]> {
    return defer(() => fromPromise(page.evaluate((pageEvalData) => {
        const collection: HTMLCollection = document.body.children;
        const results: JobInterface[] = [];
        for (let i = 0; i < collection.length; i++) {
            try {
                const item = collection.item(i)!;
                const title = item.getElementsByClassName('base-search-card__title')[0].textContent!.trim();
                const imgSrc = item.getElementsByTagName('img')[0].getAttribute('data-delayed-url') || '';
                const remoteOk: boolean = !!title.match(/remote|No office location/gi);

                const url = (
                    (item.getElementsByClassName('base-card__full-link')[0] as HTMLLinkElement)
                    || (item.getElementsByClassName('base-search-card--link')[0] as HTMLLinkElement)
                ).href;

                const companyNameAndLinkContainer = item.getElementsByClassName('base-search-card__subtitle')[0];
                const companyUrl: string | undefined = companyNameAndLinkContainer?.getElementsByTagName('a')[0]?.href;
                const companyName = companyNameAndLinkContainer.textContent!.trim();
                const companyLocation = item.getElementsByClassName('job-search-card__location')[0].textContent!.trim();

                const toDate = (dateString: string) => {
                    const [year, month, day] = dateString.split('-')
                    return new Date(parseFloat(year), parseFloat(month) - 1, parseFloat(day)    )
                }

                const dateTime = (
                    item.getElementsByClassName('job-search-card__listdate')[0]
                    || item.getElementsByClassName('job-search-card__listdate--new')[0] // less than a day. TODO: Improve precision on this case.
                ).getAttribute('datetime');
                const postedDate = toDate(dateTime as string).toISOString();


                /**
                 * Calculate minimum and maximum salary
                 *
                 * Salary HTML example to parse:
                 * <span class="job-result-card__salary-info">$65,000.00 - $90,000.00</span>
                 */
                let currency: SalaryCurrency = ''
                let salaryMin = -1;
                let salaryMax = -1;

                const salaryCurrencyMap: any = {
                    ['‚Ç¨']: 'EUR',
                    ['$']: 'USD',
                    ['¬£']: 'GBP',
                }

                const salaryInfoElem = item.getElementsByClassName('job-search-card__salary-info')[0]
                if (salaryInfoElem) {
                    const salaryInfo: string = salaryInfoElem.textContent!.trim();
                    if (salaryInfo.startsWith('‚Ç¨') || salaryInfo.startsWith('$') || salaryInfo.startsWith('¬£')) {
                        const coinSymbol = salaryInfo.charAt(0);
                        currency = salaryCurrencyMap[coinSymbol] || coinSymbol;
                    }

                    const matches = salaryInfo.match(/([0-9]|,|\.)+/g)
                    if (matches && matches[0]) {
                        // values are in USA format, so we need to remove ALL the comas
                        salaryMin = parseFloat(matches[0].replace(/,/g, ''));
                    }
                    if (matches && matches[1]) {
                        // values are in USA format, so we need to remove ALL the comas
                        salaryMax = parseFloat(matches[1].replace(/,/g, ''));
                    }
                }

                // Calculate tags
                let stackRequired: string[] = [];
                title.split(' ').concat(url.split('-')).forEach(word => {
                    if (!!word) {
                        const wordLowerCase = word.toLowerCase();
                        if (pageEvalData.stacks.includes(wordLowerCase)) {
                            stackRequired.push(wordLowerCase)
                        }
                    }
                })
                // Define uniq function here. remember that page.evaluate executes inside the browser, so we cannot easily import outside functions form other contexts
                const uniq = (_array) => _array.filter((item, pos) => _array.indexOf(item) == pos);
                stackRequired = uniq(stackRequired)

                const result: JobInterface = {
                    id: item!.children[0].getAttribute('data-entity-urn') as string,
                    city: companyLocation,
                    url: url,
                    companyUrl: companyUrl || '',
                    img: imgSrc,
                    date: new Date().toISOString(),
                    postedDate: postedDate,
                    title: title,
                    company: companyName,
                    location: companyLocation,
                    salaryCurrency: currency,
                    salaryMax: salaryMax,
                    salaryMin: salaryMin,
                    countryCode: '',
                    countryText: '',
                    descriptionHtml: '',
                    remoteOk: remoteOk,
                    stackRequired: stackRequired
                };
                console.log('result', result);

                results.push(result);
            } catch (e) {
                console.error(`Something when wrong retrieving linkedin page item: ${i} on url: ${window.location}`, e.stack);
            }
        }
        return results;
    }, {stacks})) as Observable<JobInterface[]>)
}

```

El codi proporcionat extreu efectivament tota la informaci√≥ de treball disponible de la p√†gina. Encara que el codi √©s molt est√®tic, aconsegueix la feina, que √©s t√≠pic per a codi de "scraping" web.

> En un context de programaci√≥ est√†ndard, generalment √©s aconsellable descompondre el codi en funcions m√©s petites i a√Øllades per millorar la llegibilitat i la mantenibilitat. No obstant aix√≤, quan es tracta de codi executat dins de `page.evaluate` en Puppeteer, estem una mica limitats perqu√® aquest codi s'executa en la inst√†ncia de Puppeteer (Chrome), no en el nostre entorn Node.js. Per tant, tot el codi ha de ser autocontingut dins de la crida de `page.evaluate`. L'√∫nica excepci√≥ aqu√≠ s√≥n les variables (com `stacks` en el nostre cas), que poden passar-se com a arguments a `page.evaluate`, sempre que no continguin funcions o objectes complexos que no es puguin serialitzar.

In this case, the only challenging part to scrape is the salary information, as it involves converting a text format like '$65,000.00 - $90,000.00' into separate salaryMin and salaryMax values.
Additionally, we've encapsulated the entire code within a try/catch block to gracefully handle errors. While we currently log errors to the console, it's advisable to consider implementing a mechanism to store these error logs on disk. This practice becomes particularly important because web pages often undergo changes, necessitating frequent updates to the HTML parsing code.

En aquest cas, l'√∫nic component desafiant "per escrapejar"(to scrape) √©s la informaci√≥ del salari, ja que implica convertir un format de text com '$65,000.00 - $90,000.00' en valors de salari m√≠nim i m√†xim separats.
A m√©s, hem encapsulat tot el codi dins d'un bloc try/catch per gestionar els errors de manera elegant. Encara que actualment registrem els errors a la consola, √©s aconsellable considerar la implementaci√≥ d'un mecanisme per emmagatzemar aquests registres d'error en disc. Aquesta pr√†ctica es torna particularment important perqu√® les p√†gines web sovint experimenten canvis, cosa que necessita actualitzacions freq√ºents al codi de l'an√†lisi HTML.


Finalment, √©s important notar que sempre utilitzem els operadors `defer` i `fromPromise` per convertir Promeses en Observables:

```typescript
defer(() => fromPromise(myPromise()))
```
This approach is a recommended best practice that works reliably in all scenarios. Promises are eager, whereas Observables are lazy and only initiate when someone subscribes to them. The defer operator allows us to make a Promise lazy.

Aquest enfocament √©s una millor pr√†ctica recomanada que funciona de manera fiable en tots els escenaris. Les Promeses s√≥n "eager", mentre que els Observables s√≥n "lazy" i nom√©s s'inicien quan alg√∫ s'hi subscriu. L'operador `defer` ens permet convertir a "lazy" una Promesa.


### 3- Afegir un Bucle As√≠ncron per Iterar a trav√©s de Totes les P√†gines

En l'etapa anterior, hem apr√®s com obtenir totes les dades de les ofertes de treball d'una p√†gina de LinkedIn. Ara, el que volem fer √©s utilitzar aquest codi tantes vegades com sigui possible per recopilar tantes dades com puguem. Per aconseguir-ho, primer necessitem iterar a trav√©s de totes les p√†gines disponibles:


```ts:src/linkedin.ts
export function getJobsFromPageRecursive(page: Page, searchParams: ScraperSearchParams): Observable<ScraperResult> {
    return getJobsFromLinkedinPage(page, searchParams).pipe(
        map((jobs): ScraperResult => ({jobs, searchParams} as ScraperResult)),
        catchError(error => {
            console.error('error', error);
            return of({jobs: [], searchParams})
        }),
        switchMap(({jobs}) => {
            console.log(`Linkedin - Query: ${searchParams.searchText}, Location: ${searchParams.locationText}, Page: ${searchParams.nPage}, nJobs: ${jobs.length}, url: ${urlQueryPage(searchParams)}`);
            if (jobs.length === 0) {
                return EMPTY;
            } else {
                return concat(of({jobs, searchParams}), getJobsFromPageRecursive(page, {...searchParams, nPage: searchParams.nPage++}));
            }
        })
    );
}

```
El codi anterior √©s un bucle as√≠ncron creat amb recursi√≥.

> En RxJS, no podem utilitzar un bucle amb la paraula clau `for` com ho fem amb await/async. Hem d'utilitzar un bucle recursiu en el seu lloc. Encara que inicialment pugui semblar una limitaci√≥, en un context as√≠ncron, aquest m√®tode resulta ser m√©s avantatj√≥s en nombroses situacions

Podr√≠em implementar aix√≤ utilitzant Promises en lloc d'Observables? Absolutament, aqu√≠ teniu el codi equivalent escrit amb Promises:

```typescript
export async function getJobsFromAllPages(page: Page, searchParams: ScraperSearchParams): Promise<ScraperResult> {
    const results: ScraperResult = { jobs: [], searchParams };

    try {
        while (true) {
            const jobs = await getJobsFromLinkedinPage(page, searchParams);
            console.log(`Linkedin - Query: ${searchParams.searchText}, Location: ${searchParams.locationText}, Page: ${searchParams.nPage}, nJobs: ${jobs.length}, url: ${urlQueryPage(searchParams)}`);

            results.jobs.push(...jobs);

            if (jobs.length === 0) {
                break;
            }

            searchParams.nPage++;
        }
    } catch (error) {
        console.error('Error:', error);
        results.jobs = []; // Clear the jobs in case of an error.
    }

    return results;
}

```
Aquest codi realitza accions gaireb√© id√®ntiques a les que utilitza Observables, per√≤ amb una difer√®ncia cr√≠tica: nom√©s emet quan totes les p√†gines han acabat el seu processament. En contrast, la implementaci√≥ que fa √∫s d'Observables emet despr√©s de cada p√†gina. Crear un "stream" de dades √©s vital en aquest cas perqu√® volem gestionar les ofertes de treball tan aviat com es resolguin.

Certament, podr√≠em introduir la nostra l√≤gica despr√©s de la l√≠nia:

```typescript
const jobs = await getJobsFromLinkedinPage(page, searchParams);

/* Handle the jobs here */
```
...per√≤ aix√≤ acoblaria innecess√†riament el nostre codi de "scraping" amb la part que gestiona les dades de les ofertes de treball (un cas com√∫ ser√† guardar les ofertes de treball en una base de dades).

Aix√≠ doncs, en aquest exemple veiem clarament un dels molts avantatges que els Observables ofereixen respecte a les Promeses.


### 4- Afegim un altre bucle as√≠ncron per iterar a trav√©s de tots els par√†metres de cerca especificats
Ara que sabem com iterar a trav√©s de totes les p√†gines, podem passar a l'√∫ltim pas: crear un bucle per iterar a trav√©s de diferents par√†metres de cerca.

Per aconseguir-ho, primer definirem l'estructura de dades en la qual emmagatzemarem aquests par√†metres de cerca i la denominarem `searchParamsList`:

```ts:src/data.ts
const searchParamsList: { searchText: string; locationText: string }[] = [
  { searchText: 'Angular', locationText: 'Barcelona' },
  { searchText: 'Angular', locationText: 'Madrid' },
  // ...
  { searchText: 'React', locationText: 'Barcelona' },
  { searchText: 'React', locationText: 'Madrid' },
  // ...
];
```

To iterate through the `searchParamsList` array, we essentially need to convert it from an Array to an Observable using the `fromArray` operator. Subsequently, we will use the `concatMap` operator to sequentially process each searchText and locationText pair. The power of RxJS here is that, in the case where we may want to switch from sequential to parallel processing, we just need to change the `concatMap` for a `mergeMap`. In this case, it is not recommended because we will exceed LinkedIn's rate limits, but it's something to consider in other scenarios.

Per iterar as√≠ncronament a trav√©s de l'array `searchParamsList`, b√†sicament necessitem convertir-lo d'un Array a un Observable utilitzant l'operador `fromArray`. Subseq√ºentment, utilitzarem l'operador `concatMap` per processar de manera seq√ºencial cada parell de searchText i locationText. La for√ßa de RxJS aqu√≠ √©s que, en el cas que de voler canviar de processament seq√ºencial a paral¬∑lel, simplement hem de canviar el `concatMap` per un `mergeMap`. En aquest cas no es recomana perqu√® superariem el l√≠mit de peticions (per temps) de LinkedIn, per√≤ √©s una cosa a tenir en compte en altres escenaris.

```ts:src/linkedin.ts
/**
 * Creates a new page and scrapes LinkedIn job data for each pair of searchText and locationText, recursively retrieving data until there are no more pages.
 * @param browser A Puppeteer instance
 * @returns An Observable that emits scraped job data as ScraperResult
 */
export function getJobsFromLinkedin(browser: Browser): Observable<ScraperResult> {
    // Create a new page
    const createPage = defer(() => fromPromise(browser.newPage()));

    // Iterate through search parameters and scrape jobs
    const scrapeJobs = (page: Page): Observable<ScraperResult> =>
        fromArray(searchParamsList).pipe(
            concatMap(({ searchText, locationText }) =>
                getJobsFromPageRecursive(page, { searchText, locationText, nPage: 0 })
            )
        )

    // Compose sequentially previous steps
    return createPage.pipe(switchMap(page => scrapeJobs(page)));
}
```


Aquest codi iterar√† a trav√©s de diversos par√†metres de cerca, un a la vegada, i recuperar√† ofertes de feina per a cada combinaci√≥ de `searchText` i `locationText`.

**üéâ Felicitats! Ara sou capa√ßos de fer "scraping" a LinkedIn i a qualsevol altre p√†gia web! üéâ**

Tot i aix√≤, hi ha alguns desafiaments a superar per a un "scraping" consistent a LinkedIn.



## Errors Comuns al fer "Scraping" a LinkedIn

Si executeu el codi proporcionat, r√†pidament us trobareu amb nombrosos errors de LinkedIn, fent dif√≠cil fer "scraping" amb √®xit d'una quantitat significativa d'informaci√≥. Hi ha dos errors comuns que necessitem abordar:

#### 1- Resposta "status code" 429
Aquesta resposta pot ocorrer durant el scraping, i significa que esteu fent massa sol¬∑licituds. Si us trobeu amb aquest error, considereu reduir la velocitat en qu√® es duen a terme les peticions fins que desaparegui.

#### 2- Authwall de LinkedIn
De tant en tant, LinkedIn us pot redirigir a un authwall en lloc de la p√†gina desitjada. Quan apareix l'authwall, l'√∫nica opci√≥ √©s esperar una mica m√©s abans de fer la pr√≤xima sol¬∑licitud.

### Com superar aquests errors de manera efectiva

Aquests errors han de ser gestionats a la funci√≥ `getJobsFromLinkedinPage`, ampliarem aquesta funci√≥ i separarem el codi de "scraping" html en una altra funci√≥ anomenada `getLinkedinJobsFromJobsPage`. El codi √©s aix√≠:

```ts:src/linkedin.ts
const AUTHWALL_PATH = 'linkedin.com/authwall';
const STATUS_TOO_MANY_REQUESTS = 429;
const JOB_SEARCH_SELECTOR = '.job-search-card';

function goToLinkedinJobsPageAndExtractJobs(page: Page, searchParams: ScraperSearchParams): Observable<JobInterface[]> {
    return defer(() => fromPromise(page.setExtraHTTPHeaders({'accept-language': 'en-US,en;q=0.9'})))
        .pipe(
            switchMap(() => navigateToLinkedinJobsPage(page, searchParams)),
            tap(response => checkResponseStatus(response)),
            switchMap(() => throwErrorIfAuthwall(page)),
            switchMap(() => waitForJobSearchCard(page)),
            switchMap(() => getJobsFromLinkedinPage(page)),
            retryWhen(retryStrategyByCondition({
                maxRetryAttempts: 4,
                retryConditionFn: error => error.retry === true
            })),
            map(jobs =>  Array.isArray(jobs) ? jobs : []),
            take(1)
        );
}

/**
 * Navigate to the LinkedIn search page, using the provided search parameters.
 */
function navigateToLinkedinJobsPage(page: Page, searchParams: ScraperSearchParams) {
    return defer(() => fromPromise(page.goto(urlQueryPage(searchParams), {waitUntil: 'networkidle0'})));
}

/**
 * Check the HTTP response status and throw an error if too many requests have been made.
 */
function checkResponseStatus(response: any) {
    const status = response?.status();
    if (status === STATUS_TOO_MANY_REQUESTS) {
        throw {message: 'Status 429 (Too many requests)', retry: true, status: STATUS_TOO_MANY_REQUESTS};
    }
}

/**
 * Check if the current page is an authwall and throw an error if it is.
 */
function throwErrorIfAuthwall(page: Page) {
    return getPageLocationOperator(page).pipe(tap(locationHref => {
        if (locationHref.includes(AUTHWALL_PATH)) {
            console.error('Authwall error');
            throw {message: `Linkedin authwall! locationHref: ${locationHref}`, retry: true};
        }
    }));
}

/**
 * Wait for the job search card to be visible on the page, and handle timeouts or authwalls.
 */
function waitForJobSearchCard(page: Page) {
    return defer(() => fromPromise(page.waitForSelector(JOB_SEARCH_SELECTOR, {visible: true, timeout: 5000}))).pipe(
        catchError(error => throwErrorIfAuthwall(page).pipe(tap(() => {throw error})))
    );
}
```

En aquest codi, abordem els errors esmentats anteriorment, √©s a dir, l'error de resposta 429 i el problema de l'authwall. Superar aquests errors √©s crucial per assegurar l'√®xit a llarg termini del scraping web de LinkedIn.

Per gestionar aquests errors, el codi utilitza una estrat√®gia de reintents personalitzada implementada per la funci√≥ [`retryStrategyByCondition`](https://github.com/llorenspujol/linkedin-jobs-scraper/blob/12b48449d773800bada82cbbfc09f76af5ac9289/src/scraper.utils.ts#L33):

```ts:src/scraper.utils.ts
export const retryStrategyByCondition = ({maxRetryAttempts = 3, scalingDuration = 1000, retryConditionFn = (error) => true}: {
    maxRetryAttempts?: number,
    scalingDuration?: number,
    retryConditionFn?: (error) => boolean
} = {}) => (attempts: Observable<any>) => {
    return attempts.pipe(
        mergeMap((error, i) => {
            const retryAttempt = i + 1;
            if (
                retryAttempt > maxRetryAttempts ||
                !retryConditionFn(error)
            ) {
                return throwError(error);
            }
            console.log(
                `Attempt ${retryAttempt}: retrying in ${retryAttempt *
                scalingDuration}ms`
            );
            // retry after 1s, 2s, etc...
            return timer(retryAttempt * scalingDuration);
        }),
        finalize(() => console.log('retryStrategyOnlySpecificErrors - finalized'))
    );
};
```

Aquesta estrat√®gia, b√†sicament, augmenta el temps entre cada reintent despr√©s d'un frac√†s, permetent que el codi sigui m√©s resistent quan s'enfronta a aquests errors comuns de LinkedIn. Aquest mecanisme de reintent ajuda a gestionar els desafiaments associats amb el scraping de LinkedIn i millora la fiabilitat global del proc√©s.

> Nota: √âs important ser conscient que LinkedIn pot posar a la llista negra la nostra adre√ßa IP, i simplement esperar m√©s temps pot no ser una soluci√≥ efectiva. Per mitigar aquest problema potencial i reduir l'ocurr√®ncia d'errors, una pr√†ctica recomanada √©s implementar una rotaci√≥ d'IP a intervals regulars. Com a exemple, utilitzar una VPN com a proxy i canviar peri√≤dicament entre diferents ubicacions geogr√†fiques presenta una soluci√≥ f√†cil i efectiva a aquest repte.


## Paraules Finals

El web scraping pot violar freq√ºentment els termes de servei d'un lloc web. Sempre reviseu i respecteu l'arxiu robots.txt d'un lloc web i els seus Termes de Servei. En aquest cas, aquest codi hauria de ser utilitzat NOM√âS amb fins docents i de hobby. LinkedIn prohibeix espec√≠ficament qualsevol extracci√≥ de dades del seu lloc web; podeu llegir m√©s [aqu√≠](https://www.linkedin.com/legal/crawling-terms).

Recomano utilitzar el web scraping per a l'aprenentatge, l'ensenyament i projectes de hobby. Sempre recordeu de respectar el lloc web, eviteu llan√ßar massa sol¬∑licituds i assegureu-vos que les dades s'utilitzen de manera respectuosa.

Podeu trobar tot el codi actualitzat en aquest [repositori](https://github.com/llorenspujol/linkedin-jobs-scraper)!

Bon Scraping!üï∑

